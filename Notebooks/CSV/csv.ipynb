{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bfe9deb-e570-427e-8cf3-5679798fe949",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b>Notebook Highlights:</b>\n",
    "\n",
    "--> Different ways to read csv data from object store(s3), DBFS\n",
    "\n",
    "--> Datframe functions like col, select, where, withColumn, lit\n",
    "\n",
    "--> read_files sql function\n",
    "\n",
    "--> Different ways to create temporary views in databricks.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49ac07ea-b858-4723-833e-a8c71ca1e30c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b>Supported spark csv reader options</b>\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources-csv.html\n",
    "\n",
    "<b>Work with malformed CSV records<b>\n",
    "\n",
    "When reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. For example, a field containing name of the city will not parse as an integer. The consequences depend on the mode that the parser runs in:\n",
    "\n",
    "<b>PERMISSIVE (default)</b>: nulls are inserted for fields that could not be parsed correctly\n",
    "\n",
    "<b>DROPMALFORMED</b>: drops lines that contain fields that could not be parsed\n",
    "\n",
    "<b>FAILFAST</b>: aborts the reading if any malformed data is found\n",
    "\n",
    "<b>Note : CSV Reader will return DataFrame as an output</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3132b5a-7e57-4866-bae3-032cbee17dff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "     .read \\\n",
    "     .format('csv') \\\n",
    "     .option(\"header\",'true') \\\n",
    "     .option(\"inferSchema\",'true')\\\n",
    "     .load('s3://aspad-22082024-test/2024_05_08T04_08_53Z/allergies.csv')\n",
    "\"\"\"\n",
    "     The show() function in Spark DataFrame is used to display the contents of the DataFrame in a tabular format.\n",
    "     Spark will display a certain number of rows from the DataFrame, usually the first 20 rows by default\n",
    "     Here are some common arguments you can pass \n",
    "     numRows: Specifies the number of rows to display. By default, it displays the first 20 rows.\n",
    "     truncate: Specifies whether to truncate the displayed data if it's too wide. Truncation means cutting off some characters to fit the data within the display width. By default, it's set to True\n",
    "     vertical: Specifies whether to display the output in a vertical format. By default, it's set to False, meaning the data is displayed horizontally.\n",
    "     Usage:\n",
    "     n: An alias for numRows.\n",
    "     df.show(n=100, truncate=False, vertical=True)  # Display without truncating and in vertical format\n",
    "\"\"\"\n",
    "df.show(10, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe433ea5-ddd7-4ea6-8e6e-668911cc32d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark DataFrame as a table of data with rows and columns, kind of like a spreadsheet. \n",
    "\n",
    "It's a distributed collection of data organized into named columns, similar to a relational database table. \n",
    "\n",
    "The key difference is that it can handle massive amounts of data across a cluster of computers, making it ideal for big data processing tasks. \n",
    "\n",
    "So, you can perform various operations on this data, like filtering, aggregating, joining, and analyzing, using Spark's distributed computing power. \n",
    "\n",
    "It's a powerful tool for handling large-scale data processing tasks efficiently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa50186-caea-4879-b389-d5104f45e159",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "options = {\n",
    "    \"header\":\"true\",\n",
    "    \"inferSchema\":\"true\"\n",
    "}\n",
    "df = spark \\\n",
    "     .read \\\n",
    "     .format('csv') \\\n",
    "     .options(**options) \\\n",
    "     .load('/Volumes/test/s3_test/health-care/allergies.csv')\n",
    "\n",
    "\"\"\"\n",
    "display() function is a powerful tool for visualizing and exploring data. It's particularly useful for working with Spark DataFrames in a Databricks notebook environment.\n",
    "Here's what you can do with it.\n",
    "Viewing DataFrames\n",
    "Plotting Charts\n",
    "Data Profiling\n",
    "    powerful tool for understanding the structure and characteristics of your data. It provides summary statistics and insights about the columns in your DataFrame, helping you identify patterns, anomalies, and potential issues in your data. Here's what you can expect from the data profiling feature:\n",
    "\n",
    "    Summary Statistics: Databricks automatically computes summary statistics for each column in your DataFrame, such as count, mean, standard deviation, minimum, maximum, and quantiles. This gives you a quick overview of the distribution and range of values in your data.\n",
    "\n",
    "    Data Quality Metrics: In addition to summary statistics, Databricks calculates data quality metrics for each column, such as the percentage of missing values, distinct values, and the most frequent values. This helps you assess the completeness and uniqueness of your data.\n",
    "\n",
    "    Histograms and Frequency Distributions: Databricks generates histograms and frequency distributions for numerical columns, allowing you to visualize the distribution of values and identify any outliers or unusual patterns.\n",
    "\n",
    "    Top Values: For categorical columns, Databricks displays the top values and their frequencies, giving you insights into the most common categories and their prevalence in the data.\n",
    "\n",
    "    Data Profiling Insights: Databricks provides additional insights and recommendations based on the data profiling results, highlighting potential issues or areas for further investigation. For example, it may flag columns with a high percentage of missing values or suggest data transformations to improve data quality.\n",
    "Note: display is a databricks related function doesn't work in all the spark environments\n",
    "\"\"\"\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58194c14-6129-4616-a886-4bbfc454378c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The <b>printSchema()</b> function is a method commonly used in Apache Spark to display the schema of a DataFrame. A schema in Spark defines the structure of the data, including the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20647c09-6d4c-4c93-927d-69322631a901",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.schema.__len__()\n",
    "df.schema.simpleString() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94e622e-6526-4053-b2a1-c3d1f1169e9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84dc9b7a-43eb-48da-8358-9b19c9b7ab4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.schema.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a537c9f-fb5c-47f4-a9bc-43dc080492b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.schema.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5601bdd7-1c74-476e-9914-c918658ed383",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    IntegerType,\n",
    "    DoubleType,\n",
    "    TimestampType,\n",
    ")\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"ID\", StringType(), True),\n",
    "        StructField(\"CLAIMID\", StringType(), True),\n",
    "        StructField(\"CHARGEID\", IntegerType(), True),\n",
    "        StructField(\"PATIENTID\", StringType(), True),\n",
    "        StructField(\"TYPE\", StringType(), True),\n",
    "        StructField(\"AMOUNT\", DoubleType(), True),\n",
    "        StructField(\"METHOD\", StringType(), True),\n",
    "        StructField(\"FROMDATE\", TimestampType(), True),\n",
    "        StructField(\"TODATE\", TimestampType(), True),\n",
    "        StructField(\"PLACEOFSERVICE\", StringType(), True),\n",
    "        StructField(\"PROCEDURECODE\", DoubleType(), True),\n",
    "        StructField(\"MODIFIER1\", StringType(), True),\n",
    "        StructField(\"MODIFIER2\", StringType(), True),\n",
    "        StructField(\"DIAGNOSISREF1\", IntegerType(), True),\n",
    "        StructField(\"DIAGNOSISREF2\", IntegerType(), True),\n",
    "        StructField(\"DIAGNOSISREF3\", IntegerType(), True),\n",
    "        StructField(\"DIAGNOSISREF4\", IntegerType(), True),\n",
    "        StructField(\"UNITS\", IntegerType(), True),\n",
    "        StructField(\"DEPARTMENTID\", IntegerType(), True),\n",
    "        StructField(\"NOTES\", StringType(), True),\n",
    "        StructField(\"UNITAMOUNT\", DoubleType(), True),\n",
    "        StructField(\"TRANSFEROUTID\", IntegerType(), True),\n",
    "        StructField(\"TRANSFERTYPE\", StringType(), True),\n",
    "        StructField(\"PAYMENTS\", DoubleType(), True),\n",
    "        StructField(\"ADJUSTMENTS\", IntegerType(), True),\n",
    "        StructField(\"TRANSFERS\", DoubleType(), True),\n",
    "        StructField(\"OUTSTANDING\", DoubleType(), True),\n",
    "        StructField(\"APPOINTMENTID\", StringType(), True),\n",
    "        StructField(\"LINENOTE\", StringType(), True),\n",
    "        StructField(\"PATIENTINSURANCEID\", StringType(), True),\n",
    "        StructField(\"FEESCHEDULEID\", IntegerType(), True),\n",
    "        StructField(\"PROVIDERID\", StringType(), True),\n",
    "        StructField(\"SUPERVISINGPROVIDERID\", StringType(), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(schema)\n",
    "    .load(\"/Volumes/test/s3_test/health-care/claims.csv\")\n",
    ")\n",
    "display(df)\n",
    "# df = spark \\\n",
    "#      .read \\\n",
    "#      .format('csv') \\\n",
    "#      .option(\"header\",'true') \\\n",
    "#      .option(\"inferSchema\", \"true\")\\\n",
    "#      .load('/Volumes/test/s3_test/health-care/claims.csv')\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dbaba26-1270-478b-a5c6-c497f01dbf2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When using the PERMISSIVE mode, you can enable the rescued data column to capture any data that wasn’t parsed because one or more fields in a record have one of the following issues:\n",
    "\n",
    "Absent from the provided schema.\n",
    "\n",
    "Does not match the data type of the provided schema.\n",
    "\n",
    "Has a case mismatch with the field names in the provided schema.\n",
    "\n",
    "The rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record. To remove the source file path from the rescued data column, you can set the SQL configuration spark.conf.set(\"spark.databricks.sql.rescuedDataColumn.filePath.enabled\", \"false\"). You can enable the rescued data column by setting the option rescuedDataColumn to a column name when reading data, such as _rescued_data with spark.read.option(\"rescuedDataColumn\", \"_rescued_data\").format(\"csv\").load(<path>).\n",
    "\n",
    "The CSV parser supports three modes when parsing records: PERMISSIVE, DROPMALFORMED, and FAILFAST. When used together with rescuedDataColumn, data type mismatches do not cause records to be dropped in DROPMALFORMED mode or throw an error in FAILFAST mode. Only corrupt records—that is, incomplete or malformed CSV—are dropped or throw errors.\n",
    "\n",
    "When rescuedDataColumn is used in PERMISSIVE mode, the following rules apply to corrupt records:\n",
    "\n",
    "The first row of the file (either a header row or a data row) sets the expected row length.\n",
    "\n",
    "A row with a different number of columns is considered incomplete.\n",
    "\n",
    "Data type mismatches are not considered corrupt records.\n",
    "\n",
    "Only incomplete and malformed CSV records are considered corrupt and recorded to the _corrupt_record column or badRecordsPath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e9f0278-a91d-4921-ae6c-fd37e3d527f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs\n",
    "ls /databricks-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8653947c-58a1-43f8-84af-548eaf50fa90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(spark.read.format('csv').option(\"header\",True).load('/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv'))\n",
    "display(spark.read.format('csv').option(\"header\",True).load('s3://aspad-22082024-test/2024_05_08T04_08_53Z/daimonds.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20538862-ac1e-40eb-9814-d2d21cd48d27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "How to identify good records and bad records while loading csv data into spark?\n",
    "\n",
    "The schema contains a special column _corrupt_record, which does not exist in the data. This column captures rows that did not parse correctly.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754135fd-acc2-4103-812c-a5f42d3d1c51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('carat', DoubleType(), True),\n",
    "    StructField('cut', StringType(), True),\n",
    "    StructField('color', StringType(), True),\n",
    "    StructField('clarity', StringType(), True),\n",
    "    StructField('depth', DoubleType(), True),\n",
    "    StructField('table', DoubleType(), True),\n",
    "    StructField('price', IntegerType(), True),\n",
    "    StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True),\n",
    "    StructField(\"_corrupt_record\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Adjust the reading of the CSV file to include the schema and capture corrupt records\n",
    "diamonds_with_wrong_schema = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/Volumes/test/s3_test/health-care/daimonds.csv\")\n",
    "    \n",
    "# Now, you can filter based on the `_corrupt_record` column\n",
    "goodRecords_df = diamonds_with_wrong_schema.where(\"_corrupt_record IS NULL\")\n",
    "badRecords_df = diamonds_with_wrong_schema.where(\"_corrupt_record IS NOT NULL\")\n",
    "# Good Records will be propagated to the next layers\n",
    "# Bads records will be moved the expection folders for further analysis.\n",
    "display(badRecords_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5fe1029-1938-45b9-87b2-cc4bb6bf88b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan\n",
    "\n",
    "# Define schema without the _corrupt_record column\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('carat', DoubleType(), True),\n",
    "    StructField('cut', StringType(), True),\n",
    "    StructField('color', StringType(), True),\n",
    "    StructField('clarity', StringType(), True),\n",
    "    StructField('depth', DoubleType(), True),\n",
    "    StructField('table', DoubleType(), True),\n",
    "    StructField('price', IntegerType(), True),\n",
    "    StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Load the CSV file with the schema\n",
    "diamonds_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(\"/Volumes/test/s3_test/health-care/daimonds.csv\")\n",
    "\n",
    "# Identify rows with invalid data (corrupt records) in any column except the last one\n",
    "# For example, check for invalid types, null values, or NaN in numeric fields\n",
    "badRecords_df = diamonds_df.filter(\n",
    "    (col('id').isNull()) | \n",
    "    (col('carat').isNull()) |\n",
    "    (col('carat') < 0) |  # Example of invalid value for carat\n",
    "    (col('depth').isNull()) | \n",
    "    (isnan(col('depth'))) |\n",
    "    (col('price').isNull()) |\n",
    "    (isnan(col('price'))) |\n",
    "    (col('x').isNull()) |\n",
    "    (col('y').isNull()) |\n",
    "    (col('z').isNull())\n",
    ")\n",
    "\n",
    "# Filter good records where none of the columns are corrupt\n",
    "goodRecords_df = diamonds_df.subtract(badRecords_df)\n",
    "\n",
    "# Display bad records for further analysis\n",
    "display(badRecords_df)\n",
    "\n",
    "# Optional: Display good records\n",
    "# display(goodRecords_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d77e887b-8bb6-4c11-8912-9b90a1197df3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b>select()</b> function is used to select one or more columns from a DataFrame. It allows you to create a new DataFrame containing only the columns you specify.\n",
    "\n",
    "<b>col()</b> function is used to create a Column object representing a column in a DataFrame. This function is commonly used when performing column-based operations, such as selecting, filtering, or transforming data\n",
    "\n",
    "<b>alias()</b> function is used to rename a column in a DataFrame. It allows you to create a new DataFrame with the same data but with the column renamed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b120494a-8e15-4ef3-8d2f-b8ef6e36abf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "ret_df = goodRecords_df.select('id','carat','cut','color')\n",
    "display(ret_df)\n",
    "# ------------------------------------------------------------------------\n",
    "output_df = goodRecords_df.select(col('id'),col('carat'),col('cut'),col('color'))\n",
    "display(output_df)\n",
    "# ------------------------------------------------------------------------\n",
    "output_df = goodRecords_df.select(col('id').alias(\"identifier\"),col('carat'),col('cut'),col('color'))\n",
    "display(output_df)\n",
    "# ------------------------------------------------------------------------\n",
    "columns_list = goodRecords_df.columns\n",
    "print(columns_list)\n",
    "ignore_columns = ['_corrupt_record']\n",
    "goodRecords_df  = goodRecords_df.select([col for col in columns_list if col not in ignore_columns])\n",
    "# Now you can propagate the good records to the next layers.\n",
    "display(goodRecords_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5158e086-999e-43bc-a5d3-3031875287b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<b>withColumn()</b> function is used to add, replace, or transform a column in a DataFrame. It allows you to create a new DataFrame with additional or modified columns based on existing ones.\n",
    "\n",
    "returns a new DataFrame with the specified modifications, leaving the original DataFrame unchanged.\n",
    "\n",
    "<b>withColumnRenamed()</b> function is used to rename a column in a DataFrame. It allows you to create a new DataFrame with the specified column renamed.\n",
    "\n",
    "<b>lit()</b> is a function used to create a literal value Column. It's often used when you want to add a constant value as a new column or perform transformations that involve literal values.\n",
    "\n",
    "In these examples:\n",
    "\n",
    "<b>lit(123)</b> creates a Column with a literal value of 123. This literal value can be any Python value (string, integer, float, boolean, etc.).\n",
    "\n",
    "<b>df[\"existingColumnName\"] + lit(10)</b> adds a new column \"transformedColumnName\" to the DataFrame, where each value is the sum of the values in the existing column \"existingColumnName\" and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcee0db-74dd-4371-9a5f-c0f7a28e02cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\"\"\"\n",
    "  adding new_price column\n",
    "\"\"\"\n",
    "transformed_df = goodRecords_df.withColumn('new_price',col('price')/(col('x')*col('y')*col('z')))\n",
    "display(transformed_df)\n",
    "\"\"\"\n",
    "  adding two new columns new_int_column,price_2_multiplier \n",
    "\"\"\"\n",
    "transformed_df = transformed_df\\\n",
    "                  .withColumn('new_int_column',lit(123))\\\n",
    "                  .withColumn('price_2_multiplier',col('new_price') * lit(123))\n",
    "\n",
    "display(transformed_df)\n",
    "\n",
    "transformed_df = transformed_df.withColumnRenamed('price_2_multiplier','price_2_multiplier_renamed')\n",
    "\n",
    "display(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2dd43c6-08f8-4652-867a-899139fd1487",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType\n",
    "spark.conf.set(\"spark.databricks.sql.rescuedDataColumn.filePath.enabled\", \"false\")\n",
    "schema = StructType([\n",
    "    StructField('carat', DoubleType(), True),\n",
    "    StructField('_c0', IntegerType(), True),\n",
    "    StructField('cut', StringType(), True),\n",
    "    StructField('color', StringType(), True),\n",
    "    StructField('clarity', StringType(), True),\n",
    "    StructField('depth', DoubleType(), True),\n",
    "    StructField('table', DoubleType(), True),\n",
    "    StructField('price', IntegerType(), True),\n",
    "    StructField('x', DoubleType(), True),\n",
    "    StructField('y', DoubleType(), True),\n",
    "    StructField('z', DoubleType(), True)\n",
    "])\n",
    "diamonds_with_wrong_schema = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .schema(schema)\\\n",
    "    .option(\"rescuedDataColumn\", \"_rescued_data\") \\\n",
    "    .load(\"/Volumes/test/s3_test/health-care/daimonds.csv\")\n",
    "\n",
    "# Now, you can filter based on the `_rescued_data` column\n",
    "goodRecords_df = diamonds_with_wrong_schema.where(\"_rescued_data IS  NULL\")\n",
    "badRecords_df = diamonds_with_wrong_schema.where(\"_rescued_data IS NOT NULL\")\n",
    "\n",
    "display(diamonds_with_wrong_schema)\n",
    "\n",
    "#display(spark.read.format('text').load(\"/Volumes/test/s3_test/health-care/diamonds.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7114a16a-cddd-4b35-a317-87f9a0f93490",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reads files under a provided location and returns the data in tabular form.\n",
    "\n",
    "Supports reading JSON, CSV, XML, TEXT, BINARYFILE, PARQUET, AVRO, and ORC file formats. Can detect the file format automatically and infer a unified schema across all files.\n",
    "\n",
    "<b>Syntax</b>\n",
    "\n",
    "read_files(path [, option_key => option_value ] [...])\n",
    "\n",
    "<b>Arguments</b>\n",
    "This function requires named parameter invocation for the option keys.\n",
    "\n",
    "path: A STRING with the URI of the location of the data. Supports reading from Azure Data Lake Storage Gen2 ('abfss://'), S3 (s3://) and Google Cloud Storage ('gs://'). Can contain globs. See File discovery for more details.\n",
    "\n",
    "option_key: The name of the option to configure. You need to use backticks (`) for options that contain dots (.).\n",
    "\n",
    "option_value: A constant expression to set the option to. Accepts literals and scalar functions.\n",
    "\n",
    "<b>Returns</b>\n",
    "A table comprised of the data from files read under the given path.\n",
    "\n",
    "https://docs.databricks.com/en/sql/language-manual/functions/read_files.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58aa7bf2-b9d6-4699-922f-42cf4827fb03",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM read_files('/Volumes/test/s3_test/health-care/diamonds.csv');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2fe614c-0ccc-4102-8071-7d85f67cdd30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace table claims as\n",
    "SELECT * FROM read_files(\n",
    "    '/Volumes/test/s3_test/health-care/claims.csv',\n",
    "    format => 'csv',\n",
    "    schema => 'ID string, CLAIMID string, CHARGEID integer, PATIENTID string, TYPE string, AMOUNT double, METHOD string, FROMDATE timestamp, TODATE timestamp, PLACEOFSERVICE string, PROCEDURECODE double, MODIFIER1 string, MODIFIER2 string, DIAGNOSISREF1 integer, DIAGNOSISREF2 integer, DIAGNOSISREF3 integer, DIAGNOSISREF4 integer, UNITS integer, DEPARTMENTID integer, NOTES string, UNITAMOUNT double, TRANSFEROUTID integer, TRANSFERTYPE string, PAYMENTS double, ADJUSTMENTS integer, TRANSFERS double, OUTSTANDING double, APPOINTMENTID string, LINENOTE string, PATIENTINSURANCEID string, FEESCHEDULEID integer, PROVIDERID string, SUPERVISINGPROVIDERID string',\n",
    "    header => true,\n",
    "    mode => 'PERMISSIVE',\n",
    "    rescuedDataColumn => 'rescuedcolumn')\n",
    ";\n",
    "\n",
    "select * from claims where rescuedcolumn is not null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38366a8c-57a1-45dc-8444-e6c749942d53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace  view claims_bad_records as (select * from claims where rescuedcolumn is not null);\n",
    "\n",
    "create or replace  view claims_good_records as (select * from claims where rescuedcolumn is null);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1074a9-5dc1-4d05-b707-ce62c4cf390d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "claims_bad_records_df = spark.sql(\"select * from claims_bad_records\")\n",
    "claims_good_records_df = spark.sql(\"select * from claims_good_records\")\n",
    "\n",
    "display(claims_bad_records_df)\n",
    "display(claims_good_records_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83212336-4399-4147-8546-01f8212839b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-schema.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4497bf2-787c-49cb-86d9-4097a3bd31c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS lakehouse_dev \n",
    "MANAGED LOCATION 's3://prudhvi-08052024-test/lakehouse/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438863e6-af5c-4d8a-beaa-89a8b68ba91f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Database and Schema are interchangeable\n",
    "CREATE SCHEMA IF NOT EXISTS lakehouse_dev.vidya_sankalp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "788c0a2c-cee3-4439-aae7-943e109402a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Need to create a external location in unity catalog before creating a table using CREATE TABLE syntax\n",
    "\n",
    "Note: This is applicable only when workspace is enabled with unity catalog.\n",
    "\n",
    "https://docs.databricks.com/en/connect/unity-catalog/index.html#manage-external-locations\n",
    "\n",
    "https://docs.databricks.com/en/connect/unity-catalog/external-locations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362609e6-929f-41b4-b193-b0b0b9eaf964",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS claims_transactions(\n",
    "  ID string, \n",
    "  CLAIMID string,\n",
    "  CHARGEID integer, \n",
    "  PATIENTID string, \n",
    "  TYPE string, \n",
    "  AMOUNT double, \n",
    "  METHOD string, \n",
    "  FROMDATE timestamp, \n",
    "  TODATE timestamp, \n",
    "  PLACEOFSERVICE string, \n",
    "  PROCEDURECODE double, \n",
    "  MODIFIER1 string, \n",
    "  MODIFIER2 string, \n",
    "  DIAGNOSISREF1 integer, \n",
    "  DIAGNOSISREF2 integer, \n",
    "  DIAGNOSISREF3 integer, \n",
    "  DIAGNOSISREF4 integer, \n",
    "  UNITS integer, \n",
    "  DEPARTMENTID integer, \n",
    "  NOTES string, \n",
    "  UNITAMOUNT double, \n",
    "  TRANSFEROUTID integer, \n",
    "  TRANSFERTYPE string, \n",
    "  PAYMENTS double, \n",
    "  ADJUSTMENTS integer, \n",
    "  TRANSFERS double, \n",
    "  OUTSTANDING double, \n",
    "  APPOINTMENTID string, \n",
    "  LINENOTE string, \n",
    "  PATIENTINSURANCEID string, \n",
    "  FEESCHEDULEID integer, \n",
    "  PROVIDERID string, \n",
    "  SUPERVISINGPROVIDERID string\n",
    ")\n",
    "USING CSV \n",
    "LOCATION 's3://prudhvi-08052024-test/test/claims'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb4b035e-81e5-4fb5-9017-a9c6fc6408ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from claims_transactions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55f2adc6-23c6-4d3a-9054-90cdd87f9e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-aux-describe-table.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a762402d-832f-482c-9bb9-86e8261c8eb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ef3b8a-e50a-4468-b398-c9c282f3eb02",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE EXTENDED claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26425057-83a2-4a50-9a83-227b24f41b1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "extended_table_df = spark.sql(\"DESCRIBE TABLE EXTENDED claims\")\n",
    "display(extended_table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9996a51e-16f7-4475-a4d6-eb2d45ca9908",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE SCHEMA lakehouse_dev.vidya_sankalp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8756fdc2-7a5b-4509-a5f8-d6766c332e84",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE SCHEMA EXTENDED vidya_sankalp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a401b977-4b0f-421b-827a-1c48700d7cb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can create or replace a temporary view (also known as a temporary table) to make DataFrame data accessible via SQL queries.\n",
    "\n",
    "In the below example:\n",
    "\n",
    "<b>transformed_df</b> is the DataFrame you want to create a temporary view from.\n",
    "\n",
    "<b>\"diamonds_views\"</b> is the name you want to assign to the temporary view.\n",
    "If a temporary view with the same name already exists, it will be replaced with the new DataFrame.\n",
    "\n",
    "After creating the temporary view, you can query it using SQL syntax:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c5fafc1-2ff1-4047-a838-5d6933a9c0ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transformed_df.createOrReplaceTempView(\"diamonds_views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8e74ef3-ada0-4310-85c1-8d75ee4a7d23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM diamonds_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dec3e269-9191-4840-9cd0-248ce45051ac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-view.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34b6340c-6fdf-469a-928e-7bda0a1e3a3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW IF NOT EXISTS tmp_diamonds_view\n",
    "    (id COMMENT 'Unique identification number', carat, cut, color, clarity, depth, table, price, x, y, z)\n",
    "    COMMENT 'tmporary view for diamonds table '\n",
    "    AS SELECT id,carat, cut, color, clarity, depth, table, price, x, y, z from diamonds_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1410d4e5-f795-4541-98fe-2de18469cc3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from tmp_diamonds_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6270a195-69fe-4083-ae3d-3c2e4a693323",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table diamonds(id int)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 21391013565420,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "csv",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
